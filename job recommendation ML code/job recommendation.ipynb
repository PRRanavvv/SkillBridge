{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0313b702-7e9d-48ae-8d38-54c1169d9c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 27010 jobs\n",
      "Columns: ['Unnamed: 0', 'Job Salary', 'Job Experience Required', 'Key Skills', 'Role Category', 'Functional Area', 'Industry', 'Job Title']\n",
      "Job Salary data type: object\n",
      "Job Salary sample values: [' Not Disclosed by Recruiter ', ' Not Disclosed by Recruiter ', ' Not Disclosed by Recruiter ']\n",
      "Job Experience Required data type: object\n",
      "Job Experience Required sample values: ['5 - 10 yrs', '2 - 5 yrs', '0 - 1 yrs']\n",
      "Key Skills data type: object\n",
      "Key Skills sample values: [' Media Planning| Digital Media', ' pre sales| closing| software knowledge| clients| requirements| negotiating| client| online bidding| good communication| technology', ' Computer science| Fabrication| Quality check| Intellectual property| Electronics| Support services| Research| Management| Human resource management| Research Executive']\n",
      "Creating a mock user interactions dataset...\n",
      "Created mock user interactions with 599 records\n",
      "Building content-based model...\n",
      "Building collaborative filtering model...\n",
      "Building matrix factorization model...\n",
      "\n",
      "EXAMPLE RECOMMENDATIONS:\n",
      "Content-based recommendations for job 0:\n",
      "       Job_ID                         Job Title\n",
      "3151     3151  Media Planning Executive/Manager\n",
      "18305   18305  Media Planning Executive/Manager\n",
      "5376     5376  Media Planning Executive/Manager\n",
      "\n",
      "Collaborative filtering recommendations for user 1:\n",
      "    Job_ID                                   Job Title\n",
      "1        1                     Sales Executive/Officer\n",
      "40      40                            Regional Manager\n",
      "46      46  Associate/Senior Associate -(NonTechnical)\n",
      "\n",
      "Matrix factorization recommendations for user 1:\n",
      "     Job_ID                      Job Title\n",
      "4         4               Testing Engineer\n",
      "74       74  Accounts Executive/Accountant\n",
      "373     373             Software Developer\n",
      "\n",
      "Hybrid recommendations:\n",
      "       Job_ID                         Job Title\n",
      "1           1           Sales Executive/Officer\n",
      "3151     3151  Media Planning Executive/Manager\n",
      "18305   18305  Media Planning Executive/Manager\n",
      "\n",
      "Profile-based recommendations:\n",
      "       Job_ID     Job Title\n",
      "12101   12101  Data Analyst\n",
      "9593     9593  Data Analyst\n",
      "16633   16633  Data Analyst\n",
      "Models saved successfully!\n",
      "You can now use these files with the Flask API:\n",
      "- models/job_recommender.pkl\n",
      "- models/jobs_data.pkl\n",
      "- models/user_interactions.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class JobRecommenderSystem:\n",
    "    def __init__(self, jobs_df, user_interactions_df=None):\n",
    "        \"\"\"\n",
    "        Initialize the Job Recommender System with job data and optional user interactions\n",
    "        \n",
    "        Parameters:\n",
    "        jobs_df (DataFrame): DataFrame containing job listings with columns:\n",
    "                           'Job Salary', 'Job Experience Required', 'Key Skills', \n",
    "                           'Role Category', 'Functional Area', 'Industry', 'Job Title'\n",
    "        user_interactions_df (DataFrame): Optional DataFrame containing user-job interactions\n",
    "                                        with columns: 'User_ID', 'Job_ID', 'Rating'\n",
    "        \"\"\"\n",
    "        self.jobs_df = jobs_df.copy()\n",
    "        # Adding a job ID if not present\n",
    "        if 'Job_ID' not in self.jobs_df.columns:\n",
    "            self.jobs_df['Job_ID'] = range(len(self.jobs_df))\n",
    "        \n",
    "        self.user_interactions_df = user_interactions_df\n",
    "        self.content_based_model = None\n",
    "        self.user_item_matrix = None\n",
    "        self.similarity_matrix = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.hybrid_weights = {'content': 0.5, 'collaborative': 0.5}\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess job data for recommendation\"\"\"\n",
    "        # Handle salary - convert to numeric, or create a binary indicator\n",
    "        if 'Job Salary' in self.jobs_df.columns:\n",
    "            # Try to extract numeric values from salary\n",
    "            try:\n",
    "                # Try direct conversion to numeric\n",
    "                self.jobs_df['Salary_Numeric'] = pd.to_numeric(self.jobs_df['Job Salary'], errors='coerce')\n",
    "                \n",
    "                # If all values are NaN after conversion, using a binary indicator instead\n",
    "                if self.jobs_df['Salary_Numeric'].isna().all():\n",
    "                    print(\"Warning: Could not convert any salary values to numeric. Using binary indicator instead.\")\n",
    "                    self.jobs_df['Salary_Disclosed'] = self.jobs_df['Job Salary'].apply(\n",
    "                        lambda x: 0 if str(x).lower().strip() in ['not disclosed', 'not disclosed by recruiter', 'na', 'n/a', ''] else 1\n",
    "                    )\n",
    "                else:\n",
    "                    # Filled NaNs with median of valid values\n",
    "                    median_salary = self.jobs_df['Salary_Numeric'].median()\n",
    "                    self.jobs_df['Salary_Numeric'] = self.jobs_df['Salary_Numeric'].fillna(median_salary)\n",
    "                    \n",
    "                    # Normalize the numeric salary\n",
    "                    scaler = MinMaxScaler()\n",
    "                    self.jobs_df['Normalized_Salary'] = scaler.fit_transform(self.jobs_df[['Salary_Numeric']])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing salary data - {e}. Using binary indicator instead.\")\n",
    "                self.jobs_df['Salary_Disclosed'] = self.jobs_df['Job Salary'].apply(\n",
    "                    lambda x: 0 if str(x).lower().strip() in ['not disclosed', 'not disclosed by recruiter', 'na', 'n/a', ''] else 1\n",
    "                )\n",
    "        \n",
    "        # Handling experience - convert to numeric if possible\n",
    "        if 'Job Experience Required' in self.jobs_df.columns:\n",
    "            try:\n",
    "                # Extract numeric years from experience text (e.g., \"5 - 10 yrs\" -> 7.5)\n",
    "                def extract_years(exp_text):\n",
    "                    if pd.isna(exp_text) or not isinstance(exp_text, str):\n",
    "                        return 0\n",
    "                    \n",
    "                    # Try to find patterns like \"X - Y yrs\" or \"X yrs\"\n",
    "                    exp_text = exp_text.lower().strip()\n",
    "                    if '-' in exp_text:\n",
    "                        parts = exp_text.split('-')\n",
    "                        if len(parts) == 2:\n",
    "                            try:\n",
    "                                min_exp = float(''.join(c for c in parts[0] if c.isdigit() or c == '.'))\n",
    "                                max_exp = float(''.join(c for c in parts[1] if c.isdigit() or c == '.'))\n",
    "                                return (min_exp + max_exp) / 2  # Average of min and max\n",
    "                            except ValueError:\n",
    "                                return 0\n",
    "                    else:\n",
    "                        # Try to extract a single number\n",
    "                        try:\n",
    "                            return float(''.join(c for c in exp_text if c.isdigit() or c == '.'))\n",
    "                        except ValueError:\n",
    "                            return 0\n",
    "                \n",
    "                self.jobs_df['Experience_Numeric'] = self.jobs_df['Job Experience Required'].apply(extract_years)\n",
    "                \n",
    "                # Normalize the numeric experience\n",
    "                scaler = MinMaxScaler()\n",
    "                self.jobs_df['Normalized_Experience'] = scaler.fit_transform(self.jobs_df[['Experience_Numeric']])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing experience data - {e}\")\n",
    "        \n",
    "        # Created a combined text feature for TF-IDF\n",
    "        text_features = []\n",
    "        \n",
    "        # Added all text features that exist in the dataframe\n",
    "        if 'Job Title' in self.jobs_df.columns:\n",
    "            text_features.append(self.jobs_df['Job Title'].fillna('').astype(str))\n",
    "            \n",
    "        if 'Key Skills' in self.jobs_df.columns:\n",
    "            text_features.append(self.jobs_df['Key Skills'].fillna('').astype(str))\n",
    "            \n",
    "        if 'Role Category' in self.jobs_df.columns:\n",
    "            text_features.append(self.jobs_df['Role Category'].fillna('').astype(str))\n",
    "            \n",
    "        if 'Functional Area' in self.jobs_df.columns:\n",
    "            text_features.append(self.jobs_df['Functional Area'].fillna('').astype(str))\n",
    "            \n",
    "        if 'Industry' in self.jobs_df.columns:\n",
    "            text_features.append(self.jobs_df['Industry'].fillna('').astype(str))\n",
    "        \n",
    "        # Combine all text features\n",
    "        if text_features:\n",
    "            self.jobs_df['Combined_Features'] = pd.Series(' '.join(str(val) for val in vals) \n",
    "                                                for vals in zip(*text_features))\n",
    "        else:\n",
    "            print(\"Warning: No text features found for content-based filtering\")\n",
    "            self.jobs_df['Combined_Features'] = \"\"\n",
    "    \n",
    "    def build_content_based_model(self):\n",
    "        \"\"\"Build the content-based filtering model\"\"\"\n",
    "        self.preprocess_data()\n",
    "        \n",
    "        # Create TF-IDF vectors for job descriptions\n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        self.tfidf_matrix = tfidf.fit_transform(self.jobs_df['Combined_Features'])\n",
    "        \n",
    "        self.content_based_model = {\n",
    "            'tfidf_matrix': self.tfidf_matrix,\n",
    "            'job_indices': {idx: job_id for idx, job_id in enumerate(self.jobs_df['Job_ID'])}\n",
    "        }\n",
    "        \n",
    "        return self.content_based_model\n",
    "    \n",
    "    def build_collaborative_model(self):\n",
    "        \"\"\"\n",
    "        Build collaborative filtering model using user-item matrix and item-item similarity\n",
    "        \"\"\"\n",
    "        if self.user_interactions_df is None:\n",
    "            print(\"No user interaction data available for collaborative filtering\")\n",
    "            return None\n",
    "        \n",
    "        # Create user-item matrix\n",
    "        # Convert ratings to a user-item matrix\n",
    "        user_item_matrix = pd.pivot_table(\n",
    "            self.user_interactions_df,\n",
    "            values='Rating',\n",
    "            index='User_ID',\n",
    "            columns='Job_ID',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Store the user-item matrix\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        \n",
    "        # Calculate item-item similarity matrix using cosine similarity\n",
    "        # Transpose the matrix to get item-item similarity\n",
    "        item_item_similarity = cosine_similarity(user_item_matrix.T)\n",
    "        self.similarity_matrix = pd.DataFrame(\n",
    "            item_item_similarity,\n",
    "            index=user_item_matrix.columns,\n",
    "            columns=user_item_matrix.columns\n",
    "        )\n",
    "        \n",
    "        return self.similarity_matrix\n",
    "    \n",
    "    def get_content_based_recommendations(self, job_id, top_n=5):\n",
    "        \"\"\"Get content-based job recommendations for a given job ID\"\"\"\n",
    "        if self.content_based_model is None:\n",
    "            self.build_content_based_model()\n",
    "        \n",
    "        # Check if job_id exists in our data\n",
    "        if job_id not in self.jobs_df['Job_ID'].values:\n",
    "            print(f\"Job ID {job_id} not found in the dataset\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Find the index of the job in our dataset\n",
    "        job_idx = self.jobs_df[self.jobs_df['Job_ID'] == job_id].index[0]\n",
    "        \n",
    "        # Calculate cosine similarity between this job and all others\n",
    "        cosine_similarities = cosine_similarity(self.tfidf_matrix[job_idx:job_idx+1], self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Get the indices of the top N most similar jobs (excluding the input job)\n",
    "        similar_indices = cosine_similarities.argsort()[::-1]\n",
    "        similar_indices = [idx for idx in similar_indices if idx != job_idx][:top_n]\n",
    "        \n",
    "        # Return the similar jobs\n",
    "        return self.jobs_df.iloc[similar_indices]\n",
    "    \n",
    "    def get_collaborative_recommendations(self, user_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get collaborative filtering recommendations for a user using item-based CF\n",
    "        \"\"\"\n",
    "        if self.similarity_matrix is None:\n",
    "            if self.user_interactions_df is None:\n",
    "                print(\"Cannot provide collaborative recommendations without user interaction data\")\n",
    "                return pd.DataFrame()\n",
    "            self.build_collaborative_model()\n",
    "        \n",
    "        # Check if user exists in the user-item matrix\n",
    "        if user_id not in self.user_item_matrix.index:\n",
    "            print(f\"User {user_id} not found in interaction data\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get the user's ratings\n",
    "        user_ratings = self.user_item_matrix.loc[user_id]\n",
    "        \n",
    "        # Jobs the user has already rated\n",
    "        rated_jobs = user_ratings[user_ratings > 0].index\n",
    "        \n",
    "        # Initialize a dictionary to store the predicted ratings\n",
    "        predicted_ratings = {}\n",
    "        \n",
    "        # For each job the user hasn't rated\n",
    "        for job_id in self.similarity_matrix.columns:\n",
    "            if job_id not in rated_jobs:\n",
    "                # Get similar jobs that the user has rated\n",
    "                similar_jobs = self.similarity_matrix[job_id]\n",
    "                similar_jobs_rated = similar_jobs[rated_jobs]\n",
    "                \n",
    "                # If there are similar jobs the user has rated\n",
    "                if len(similar_jobs_rated) > 0:\n",
    "                    # Calculate the weighted average rating\n",
    "                    numerator = sum(similar_jobs_rated * user_ratings[rated_jobs])\n",
    "                    denominator = sum(abs(similar_jobs_rated))\n",
    "                    \n",
    "                    if denominator > 0:\n",
    "                        predicted_ratings[job_id] = numerator / denominator\n",
    "                    else:\n",
    "                        predicted_ratings[job_id] = 0\n",
    "        \n",
    "        # Sort jobs by predicted rating\n",
    "        sorted_predictions = sorted(predicted_ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top N job IDs\n",
    "        top_job_ids = [job_id for job_id, _ in sorted_predictions[:top_n]]\n",
    "        \n",
    "        # Return the top N recommended jobs\n",
    "        return self.jobs_df[self.jobs_df['Job_ID'].isin(top_job_ids)]\n",
    "    \n",
    "    def matrix_factorization(self, R, P=None, Q=None, K=10, steps=50, alpha=0.01, beta=0.02):\n",
    "        \"\"\"\n",
    "        Matrix Factorization using Gradient Descent\n",
    "        Parameters:\n",
    "        R (ndarray): User-item rating matrix\n",
    "        P (ndarray): User features matrix\n",
    "        Q (ndarray): Item features matrix\n",
    "        K (int): Number of latent features\n",
    "        steps (int): Number of iterations\n",
    "        alpha (float): Learning rate\n",
    "        beta (float): Regularization parameter\n",
    "        \n",
    "        Returns:\n",
    "        P (ndarray): Updated user features matrix\n",
    "        Q (ndarray): Updated item features matrix\n",
    "        \"\"\"\n",
    "        # Get dimensions of the matrix\n",
    "        M, N = R.shape\n",
    "        \n",
    "        # Initialize P and Q if not provided\n",
    "        if P is None:\n",
    "            P = np.random.rand(M, K)  # User features\n",
    "        if Q is None:\n",
    "            Q = np.random.rand(N, K)  # Item features\n",
    "        \n",
    "        # Create a mask for non-zero entries\n",
    "        mask = (R > 0).astype(float)\n",
    "        \n",
    "        # Matrix factorization using Gradient Descent\n",
    "        for step in range(steps):\n",
    "            for i in range(M):\n",
    "                for j in range(N):\n",
    "                    if mask[i, j] > 0:  # Only for non-zero entries\n",
    "                        # Compute error\n",
    "                        eij = R[i, j] - np.dot(P[i,:], Q[j,:].T)\n",
    "                        \n",
    "                        # Update P and Q\n",
    "                        for k in range(K):\n",
    "                            P[i, k] += alpha * (2 * eij * Q[j, k] - beta * P[i, k])\n",
    "                            Q[j, k] += alpha * (2 * eij * P[i, k] - beta * Q[j, k])\n",
    "            \n",
    "            # Compute current RMSE\n",
    "            error = 0\n",
    "            count = 0\n",
    "            for i in range(M):\n",
    "                for j in range(N):\n",
    "                    if mask[i, j] > 0:\n",
    "                        error += (R[i, j] - np.dot(P[i,:], Q[j,:].T)) ** 2\n",
    "                        count += 1\n",
    "            rmse = np.sqrt(error / count) if count > 0 else 0\n",
    "            \n",
    "            # Early stopping if error is small enough\n",
    "            if rmse < 0.001:\n",
    "                break\n",
    "        \n",
    "        return P, Q\n",
    "    \n",
    "    def build_mf_collaborative_model(self, k=10, steps=50):\n",
    "        \"\"\"\n",
    "        Build collaborative filtering model using matrix factorization\n",
    "        \"\"\"\n",
    "        if self.user_interactions_df is None:\n",
    "            print(\"No user interaction data available for collaborative filtering\")\n",
    "            return None\n",
    "        \n",
    "        # Create user-item matrix\n",
    "        user_item_matrix = pd.pivot_table(\n",
    "            self.user_interactions_df,\n",
    "            values='Rating',\n",
    "            index='User_ID',\n",
    "            columns='Job_ID',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Store the user-item matrix and indices\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.user_indices = {i: user_id for i, user_id in enumerate(user_item_matrix.index)}\n",
    "        self.job_indices = {i: job_id for i, job_id in enumerate(user_item_matrix.columns)}\n",
    "        self.reverse_user_indices = {user_id: i for i, user_id in self.user_indices.items()}\n",
    "        self.reverse_job_indices = {job_id: i for i, job_id in self.job_indices.items()}\n",
    "        \n",
    "        # Matrix factorization\n",
    "        R = user_item_matrix.values\n",
    "        self.P, self.Q = self.matrix_factorization(R, K=k, steps=steps)\n",
    "        \n",
    "        return self.P, self.Q\n",
    "    \n",
    "    def get_mf_recommendations(self, user_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get collaborative filtering recommendations using matrix factorization\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'P') or not hasattr(self, 'Q'):\n",
    "            self.build_mf_collaborative_model()\n",
    "        \n",
    "        # Check if user exists\n",
    "        if user_id not in self.reverse_user_indices:\n",
    "            print(f\"User {user_id} not found in interaction data\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get user index\n",
    "        user_idx = self.reverse_user_indices[user_id]\n",
    "        \n",
    "        # Get user's actual ratings\n",
    "        user_ratings = self.user_item_matrix.loc[user_id]\n",
    "        rated_jobs = user_ratings[user_ratings > 0].index.tolist()\n",
    "        \n",
    "        # Predict ratings for all jobs\n",
    "        user_features = self.P[user_idx, :]\n",
    "        predicted_ratings = {}\n",
    "        \n",
    "        for job_idx, job_id in self.job_indices.items():\n",
    "            if job_id not in rated_jobs:  # Only recommend jobs the user hasn't rated\n",
    "                job_features = self.Q[job_idx, :]\n",
    "                predicted_rating = np.dot(user_features, job_features.T)\n",
    "                predicted_ratings[job_id] = predicted_rating\n",
    "        \n",
    "        # Sort jobs by predicted rating\n",
    "        sorted_predictions = sorted(predicted_ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top N job IDs\n",
    "        top_job_ids = [job_id for job_id, _ in sorted_predictions[:top_n]]\n",
    "        \n",
    "        # Return the top N recommended jobs\n",
    "        return self.jobs_df[self.jobs_df['Job_ID'].isin(top_job_ids)]\n",
    "    \n",
    "    def get_hybrid_recommendations(self, user_id, job_id=None, top_n=5, mf=False):\n",
    "        \"\"\"\n",
    "        Get hybrid recommendations using both content-based and collaborative filtering\n",
    "        \n",
    "        Parameters:\n",
    "        user_id: User ID for collaborative filtering\n",
    "        job_id: Optional job ID for content-based filtering (e.g., job the user is currently viewing)\n",
    "        top_n: Number of recommendations to return\n",
    "        mf: Whether to use matrix factorization for collaborative filtering\n",
    "        \"\"\"\n",
    "        content_recommendations = pd.DataFrame()\n",
    "        collab_recommendations = pd.DataFrame()\n",
    "        \n",
    "        # Get content-based recommendations if job_id is provided\n",
    "        if job_id is not None:\n",
    "            content_recommendations = self.get_content_based_recommendations(job_id, top_n=top_n)\n",
    "        \n",
    "        # Get collaborative filtering recommendations\n",
    "        if self.user_interactions_df is not None:\n",
    "            if mf:\n",
    "                collab_recommendations = self.get_mf_recommendations(user_id, top_n=top_n)\n",
    "            else:\n",
    "                collab_recommendations = self.get_collaborative_recommendations(user_id, top_n=top_n)\n",
    "        \n",
    "        # If we have both types of recommendations, combine them with weights\n",
    "        if not content_recommendations.empty and not collab_recommendations.empty:\n",
    "            # Create a score for each recommendation type\n",
    "            content_jobs = set(content_recommendations['Job_ID'])\n",
    "            collab_jobs = set(collab_recommendations['Job_ID'])\n",
    "            \n",
    "            # Combine the job sets\n",
    "            all_recommended_jobs = content_jobs.union(collab_jobs)\n",
    "            \n",
    "            # Calculate hybrid scores\n",
    "            hybrid_scores = {}\n",
    "            for job in all_recommended_jobs:\n",
    "                score = 0\n",
    "                if job in content_jobs:\n",
    "                    # Give more weight to higher-ranked jobs in content recommendations\n",
    "                    rank = list(content_recommendations['Job_ID']).index(job)\n",
    "                    score += self.hybrid_weights['content'] * (1 - (rank / len(content_jobs)))\n",
    "                \n",
    "                if job in collab_jobs:\n",
    "                    # Give more weight to higher-ranked jobs in collaborative recommendations\n",
    "                    rank = list(collab_recommendations['Job_ID']).index(job)\n",
    "                    score += self.hybrid_weights['collaborative'] * (1 - (rank / len(collab_jobs)))\n",
    "                \n",
    "                hybrid_scores[job] = score\n",
    "            \n",
    "            # Sort by hybrid score and get top_n\n",
    "            sorted_jobs = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "            top_job_ids = [job_id for job_id, _ in sorted_jobs]\n",
    "            \n",
    "            return self.jobs_df[self.jobs_df['Job_ID'].isin(top_job_ids)]\n",
    "        \n",
    "        # If we only have one type of recommendations, return that\n",
    "        elif not content_recommendations.empty:\n",
    "            return content_recommendations\n",
    "        elif not collab_recommendations.empty:\n",
    "            return collab_recommendations\n",
    "        else:\n",
    "            # If we have no recommendations, return top jobs by some other metric\n",
    "            # Try using salary if available, otherwise just return first top_n jobs\n",
    "            if 'Normalized_Salary' in self.jobs_df.columns:\n",
    "                return self.jobs_df.sort_values('Normalized_Salary', ascending=False).head(top_n)\n",
    "            else:\n",
    "                return self.jobs_df.head(top_n)\n",
    "    \n",
    "    def recommend_jobs_for_user_profile(self, user_profile, top_n=5):\n",
    "        \"\"\"\n",
    "        Recommend jobs based on user profile (skills, experience, etc.)\n",
    "        \n",
    "        Parameters:\n",
    "        user_profile: Dictionary containing user profile information with keys like\n",
    "                      'Skills', 'Experience', 'Role Category', 'Industry', etc.\n",
    "        top_n: Number of recommendations to return\n",
    "        \"\"\"\n",
    "        if self.content_based_model is None:\n",
    "            self.build_content_based_model()\n",
    "        \n",
    "        # Create a pseudo-job entry from the user profile\n",
    "        pseudo_job = {\n",
    "            'Job Title': user_profile.get('Desired Job Title', ''),\n",
    "            'Key Skills': user_profile.get('Skills', ''),\n",
    "            'Role Category': user_profile.get('Role Category', ''),\n",
    "            'Functional Area': user_profile.get('Functional Area', ''),\n",
    "            'Industry': user_profile.get('Industry', ''),\n",
    "            'Job Experience Required': user_profile.get('Experience', 0),\n",
    "            'Job Salary': user_profile.get('Expected Salary', 0)\n",
    "        }\n",
    "        \n",
    "        # Combine text features as we did for the dataset\n",
    "        text_features = [\n",
    "            str(pseudo_job['Job Title']),\n",
    "            str(pseudo_job['Key Skills']),\n",
    "            str(pseudo_job['Role Category']),\n",
    "            str(pseudo_job['Functional Area']),\n",
    "            str(pseudo_job['Industry'])\n",
    "        ]\n",
    "        \n",
    "        pseudo_job_features = ' '.join(text_features)\n",
    "        \n",
    "        # Create TF-IDF vector for the pseudo job\n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        tfidf.fit(self.jobs_df['Combined_Features'])\n",
    "        jobs_tfidf = tfidf.transform(self.jobs_df['Combined_Features'])\n",
    "        pseudo_job_tfidf = tfidf.transform([pseudo_job_features])\n",
    "        \n",
    "        # Calculate cosine similarity with all jobs\n",
    "        cosine_similarities = cosine_similarity(pseudo_job_tfidf, jobs_tfidf).flatten()\n",
    "        \n",
    "        # Get indices of top N similar jobs\n",
    "        similar_indices = cosine_similarities.argsort()[::-1][:top_n]\n",
    "        \n",
    "        # Return the similar jobs\n",
    "        return self.jobs_df.iloc[similar_indices]\n",
    "\n",
    "\n",
    "# Example usage with mock user interactions\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your dataset\n",
    "    try:\n",
    "        jobs_df = pd.read_csv('Downloads/jobs.csv')\n",
    "        \n",
    "        # Display some info about the dataset\n",
    "        print(f\"Dataset loaded with {len(jobs_df)} jobs\")\n",
    "        print(f\"Columns: {jobs_df.columns.tolist()}\")\n",
    "        \n",
    "        # Check data types for key columns\n",
    "        for col in ['Job Salary', 'Job Experience Required', 'Key Skills']:\n",
    "            if col in jobs_df.columns:\n",
    "                print(f\"{col} data type: {jobs_df[col].dtype}\")\n",
    "                print(f\"{col} sample values: {jobs_df[col].head(3).tolist()}\")\n",
    "        \n",
    "        # Create a mock user interactions dataset\n",
    "        print(\"Creating a mock user interactions dataset...\")\n",
    "        \n",
    "        # Ensure jobs have Job_ID\n",
    "        if 'Job_ID' not in jobs_df.columns:\n",
    "            jobs_df['Job_ID'] = range(len(jobs_df))\n",
    "        \n",
    "        # Create a sample of job IDs to use in interactions (limit to 500 for efficiency)\n",
    "        sample_size = min(500, len(jobs_df))\n",
    "        job_ids = jobs_df['Job_ID'].iloc[:sample_size].tolist()\n",
    "        \n",
    "        # Create 50 mock users\n",
    "        num_users = 50\n",
    "        user_ids = list(range(1, num_users + 1))\n",
    "        \n",
    "        # Generate random interactions\n",
    "        sample_interactions = []\n",
    "        for user_id in user_ids:\n",
    "            # Each user interacts with 5-20 random jobs\n",
    "            num_interactions = np.random.randint(5, 21)\n",
    "            selected_job_ids = np.random.choice(job_ids, size=min(num_interactions, len(job_ids)), replace=False)\n",
    "            \n",
    "            for job_id in selected_job_ids:\n",
    "                # Generate a random rating between 1 and 5\n",
    "                rating = np.random.randint(1, 6)\n",
    "                sample_interactions.append({\n",
    "                    'User_ID': user_id,\n",
    "                    'Job_ID': job_id,\n",
    "                    'Rating': rating\n",
    "                })\n",
    "        \n",
    "        user_interactions_df = pd.DataFrame(sample_interactions)\n",
    "        print(f\"Created mock user interactions with {len(user_interactions_df)} records\")\n",
    "        \n",
    "        # Initialize the recommender system\n",
    "        recommender = JobRecommenderSystem(jobs_df, user_interactions_df)\n",
    "        \n",
    "        # Build models\n",
    "        print(\"Building content-based model...\")\n",
    "        recommender.build_content_based_model()\n",
    "        \n",
    "        print(\"Building collaborative filtering model...\")\n",
    "        recommender.build_collaborative_model()\n",
    "        \n",
    "        # Alternatively, use matrix factorization (smaller number of steps for demo)\n",
    "        print(\"Building matrix factorization model...\")\n",
    "        recommender.build_mf_collaborative_model(k=10, steps=20)\n",
    "        \n",
    "        # Example of how to get recommendations\n",
    "        # Use an actual job_id from the dataset\n",
    "        sample_job_id = jobs_df['Job_ID'].iloc[0]\n",
    "        \n",
    "        # Use an actual user_id from the interactions\n",
    "        sample_user_id = user_interactions_df['User_ID'].iloc[0]\n",
    "        \n",
    "        print(\"\\nEXAMPLE RECOMMENDATIONS:\")\n",
    "        print(f\"Content-based recommendations for job {sample_job_id}:\")\n",
    "        content_recs = recommender.get_content_based_recommendations(sample_job_id, top_n=3)\n",
    "        if not content_recs.empty:\n",
    "            if 'Job Title' in content_recs.columns:\n",
    "                print(content_recs[['Job_ID', 'Job Title']].head(3))\n",
    "            else:\n",
    "                print(content_recs[['Job_ID']].head(3))\n",
    "        else:\n",
    "            print(\"No content-based recommendations found\")\n",
    "        \n",
    "        print(f\"\\nCollaborative filtering recommendations for user {sample_user_id}:\")\n",
    "        collab_recs = recommender.get_collaborative_recommendations(sample_user_id, top_n=3)\n",
    "        if not collab_recs.empty:\n",
    "            if 'Job Title' in collab_recs.columns:\n",
    "                print(collab_recs[['Job_ID', 'Job Title']].head(3))\n",
    "            else:\n",
    "                print(collab_recs[['Job_ID']].head(3))\n",
    "        else:\n",
    "            print(\"No collaborative filtering recommendations found\")\n",
    "        \n",
    "        print(f\"\\nMatrix factorization recommendations for user {sample_user_id}:\")\n",
    "        mf_recs = recommender.get_mf_recommendations(sample_user_id, top_n=3)\n",
    "        if not mf_recs.empty:\n",
    "            if 'Job Title' in mf_recs.columns:\n",
    "                print(mf_recs[['Job_ID', 'Job Title']].head(3))\n",
    "            else:\n",
    "                print(mf_recs[['Job_ID']].head(3))\n",
    "        else:\n",
    "            print(\"No matrix factorization recommendations found\")\n",
    "        \n",
    "        print(\"\\nHybrid recommendations:\")\n",
    "        hybrid_recs = recommender.get_hybrid_recommendations(sample_user_id, sample_job_id, top_n=3)\n",
    "        if not hybrid_recs.empty:\n",
    "            if 'Job Title' in hybrid_recs.columns:\n",
    "                print(hybrid_recs[['Job_ID', 'Job Title']].head(3))\n",
    "            else:\n",
    "                print(hybrid_recs[['Job_ID']].head(3))\n",
    "        else:\n",
    "            print(\"No hybrid recommendations found\")\n",
    "            \n",
    "        # User profile example\n",
    "        user_profile = {\n",
    "            'Skills': 'Python, Data Analysis, Machine Learning',\n",
    "            'Experience': 3,\n",
    "            'Role Category': 'Data Science',\n",
    "            'Industry': 'Technology',\n",
    "            'Functional Area': 'Analytics'\n",
    "        }\n",
    "        print(\"\\nProfile-based recommendations:\")\n",
    "        profile_recs = recommender.recommend_jobs_for_user_profile(user_profile, top_n=3)\n",
    "        if not profile_recs.empty:\n",
    "            if 'Job Title' in profile_recs.columns:\n",
    "                print(profile_recs[['Job_ID', 'Job Title']].head(3))\n",
    "            else:\n",
    "                print(profile_recs[['Job_ID']].head(3))\n",
    "        else:\n",
    "            print(\"No profile-based recommendations found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in job recommender: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "model = recommender\n",
    "def predict_recommendations(user_data, num_recommendations=5):\n",
    "    \"\"\"\n",
    "    Custom prediction function for Flask API integration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract user profile from the input data\n",
    "        user_profile = {\n",
    "            'Skills': user_data.get('skills', ''),\n",
    "            'Experience': user_data.get('experience', 0),\n",
    "            'Role Category': user_data.get('role_category', ''),\n",
    "            'Industry': user_data.get('industry', ''),\n",
    "            'Functional Area': user_data.get('functional_area', ''),\n",
    "            'Desired Job Title': user_data.get('job_title', '')\n",
    "        }\n",
    "        \n",
    "        # Use profile-based recommendations\n",
    "        recommendations_df = recommender.recommend_jobs_for_user_profile(\n",
    "            user_profile, \n",
    "            top_n=num_recommendations\n",
    "        )\n",
    "        \n",
    "        # Format recommendations for API response\n",
    "        recommendations = []\n",
    "        for idx, row in recommendations_df.iterrows():\n",
    "            recommendations.append({\n",
    "                'job_id': int(row['Job_ID']),\n",
    "                'job_title': str(row.get('Job Title', 'N/A')),\n",
    "                'industry': str(row.get('Industry', 'N/A')),\n",
    "                'functional_area': str(row.get('Functional Area', 'N/A')),\n",
    "                'experience_required': str(row.get('Job Experience Required', 'N/A')),\n",
    "                'key_skills': str(row.get('Key Skills', 'N/A')),\n",
    "                'rank': len(recommendations) + 1\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in predict_recommendations: {e}\")\n",
    "        return []\n",
    "\n",
    "# Save the model and related objects for the Flask API\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create a models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the recommender system\n",
    "with open(r\"C:\\Users\\rawat\\Downloads\\model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Save the jobs dataframe\n",
    "with open('models/jobs_data.pkl', 'wb') as f:\n",
    "    pickle.dump(jobs_df, f)\n",
    "\n",
    "# Save user interactions if available\n",
    "if 'user_interactions_df' in locals():\n",
    "    with open('models/user_interactions.pkl', 'wb') as f:\n",
    "        pickle.dump(user_interactions_df, f)\n",
    "\n",
    "print(\"Models saved successfully!\")\n",
    "print(\"You can now use these files with the Flask API:\")\n",
    "print(\"- models/job_recommender.pkl\")\n",
    "print(\"- models/jobs_data.pkl\")\n",
    "print(\"- models/user_interactions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803c9d4-2441-433a-ba20-368b5bf23567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
